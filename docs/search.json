[
  {
    "objectID": "posts/001-RMSE/index.html",
    "href": "posts/001-RMSE/index.html",
    "title": "What does it mean to have an RMSE of 1.2? How wrong is it?",
    "section": "",
    "text": "I recently participated in the “Emotion Physiology and Experience Collaboration (EPiC)” competition, which involved predicting 30-seconds worth of continuous self-reported valence and arousal ratings of participants given their physiological data. You could use any model and the evaluation metric was the root mean squared error (RMSE).\nLike a good scientist, I started with browsing research and conference papers in the emotion prediction space to get a feel for what existing models were being used and how good they were at the task. One interesting thing that I saw was that the RMSEs reported in papers with complex neutral network models only slightly beat out our trusty old friend the linear regression model, with the “best fitting model” quoted as having an RMSE of 1.2. But what is RMSE anyway and what does an RMSE of 1.2 really mean?\nIn this post, I will use a subset of the EPiC dataset to demonstrate how to calculate RMSE and how to interpret it. We will use a training set to train a model and a test set to evaluate our predictions."
  },
  {
    "objectID": "posts/001-RMSE/index.html#load-in-data",
    "href": "posts/001-RMSE/index.html#load-in-data",
    "title": "What does it mean to have an RMSE of 1.2? How wrong is it?",
    "section": "Load in data",
    "text": "Load in data\nThe dataset that was used in the EPiC competition came from a public dataset called CASE, which stands for Continuously Annotated Signals of Emotion.\nThe original CASE dataset contained data from 30 participants. Each participant watched 8 videos that were chosen to elicit one of 4 emotional states (2 videos per emotional state), corresponding to different corners of the valence-arousal space: amusement (positive valence, high arousal), fear (negative valence, high arousal), boredom (negative valence, low arousal) and relaxation (positive valence, low arousal). Participants used a joystick to annotate valence and arousal in a 2D grid, which were collected at a rate of 20Hz (i.e. every 50ms). The dataset also included 8 physiological measures, each sampled at a rate of 1000Hz: electrocardiography (ECG), blood volume pulse (BVP), galvanic skin response (GSR), respiration (RSP), skin temperature (SKT) and electromyography (EMG) measuring the activity of three facial and back muscles.\nFor the purposes of this post, I subsetted a portion of the original dataset (and split it into train and test sets) and downsampled the physiological data to match the emotion ratings at every 50ms. I also exported it as a csv file and that is what we will be using here. (I hope you can appreciate that I had to do a lot of data cleaning and wrangling to get the data into this clean format! My biggest takeaway from participating in this competition was that 75% was data wrangling and preprocessing, 20% was waiting for your model to train, and 5% was actually building the model. lol just kidding… but not really.)\n\n\nCode\n# load in data\nlibrary(readr)\ntrain_dataset <- read_csv(\"https://jamie-chiu.github.io/Dear (Data) Diary/posts/001-RMSE/training_dataset.csv\")\ntest_dataset <- read_csv(\"https://jamie-chiu.github.io/Dear (Data) Diary/posts/001-RMSE/test_dataset.csv\")"
  },
  {
    "objectID": "posts/001-RMSE/index.html#explore-the-data",
    "href": "posts/001-RMSE/index.html#explore-the-data",
    "title": "What does it mean to have an RMSE of 1.2? How wrong is it?",
    "section": "Explore the data",
    "text": "Explore the data\nLet’s take a look at the training dataset.\n\n\nCode\nlibrary(tidyverse)\nlibrary(knitr)\n\n# display first few rows\nhead(train_dataset) %>%\n    select(-fold) %>%\n    knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubject\ntime\necg\nbvp\ngsr\nrsp\nskt\nemg_zygo\nemg_coru\nemg_trap\nvalence\narousal\nvideo\n\n\n\n\n0\n0\n1.004\n36.758\n6.183\n32.895\n25.232\n5.933\n7.986\n7.615\n5.000\n5.000\nscary-1\n\n\n0\n100\n0.849\n37.633\n6.209\n33.156\n25.242\n6.056\n8.109\n7.289\n5.000\n5.000\nscary-1\n\n\n0\n1000\n0.826\n37.561\n6.202\n32.914\n25.242\n6.302\n7.782\n7.905\n5.000\n5.000\nscary-1\n\n\n0\n10000\n0.886\n36.893\n6.202\n33.282\n25.232\n5.974\n8.274\n8.601\n5.000\n5.000\nscary-1\n\n\n0\n100000\n0.596\n37.455\n9.549\n31.385\n25.168\n6.837\n7.864\n8.726\n3.364\n5.715\nscary-1\n\n\n0\n100050\n0.557\n37.251\n9.518\n31.385\n25.172\n6.836\n7.862\n7.616\n3.364\n5.715\nscary-1\n\n\n\n\n\nHow many train and test subjects do we have?\n\n\nCode\n# count number of subjects\nunique(train_dataset$subject) %>%\n    length() %>%\n    paste0(\"Number of train subjects: \", .)\n\n\n[1] \"Number of train subjects: 8\"\n\n\nCode\nunique(test_dataset$subject) %>%\n    length() %>%\n    paste0(\"Number of test subjects: \", .)\n\n\n[1] \"Number of test subjects: 4\"\n\n\nHow many observations do we have for each subject?\n\n\nCode\n# count number of observations per subject per video\ntrain_dataset %>%\n    group_by(video) %>%\n    summarise(length = (n()/8)*0.05,\n              obs = n()/8) %>%\n    knitr::kable(\n        # reorder columns \n        col.names = c(\"Video\", \"Length of video (seconds)\", \"Number of observations\"),\n        caption = \"Number of training observations per video\",\n        digits = 0\n    )\n\n\n\nNumber of training observations per video\n\n\nVideo\nLength of video (seconds)\nNumber of observations\n\n\n\n\namusing-1\n161\n3217\n\n\namusing-2\n144\n2873\n\n\nboring-1\n69\n1376\n\n\nboring-2\n126\n2516\n\n\nrelaxing-1\n105\n2104\n\n\nrelaxing-2\n107\n2149\n\n\nscary-1\n177\n3531\n\n\nscary-2\n103\n2067\n\n\n\n\n\nLet’s take a look at one participant’s physiological measures and emotion annotations for a scary video.\n\n\nCode\n# plotting one participant's data\nlibrary(patchwork) # for arranging plots\n\n# define physiological measures\nmeasures <- c(\"ecg\", \"bvp\", \"gsr\", \"rsp\", \"skt\", \"emg_zygo\", \"emg_coru\", \"emg_trap\", \"valence\", \"arousal\")\n\n# create list to store plots\nplots <- list()\n\n# iterate over measures and create plots\nfor (measure in measures) {\n  plot <- train_dataset %>% \n    filter(subject == 0) %>%\n    filter(video == \"scary-2\") %>%\n    ggplot(aes(x=time, y=!!sym(measure), color=video)) +\n    geom_point(size=0.2, alpha=0.2) +\n    # remove legend\n    theme(legend.position=\"none\") +\n    # remove x-axis label\n    xlab(NULL) +\n    # y-axis to have 2 decimal places\n    scale_y_continuous(labels = scales::number_format(accuracy = 0.1))\n  # add plot to list\n  plots[[measure]] <- plot\n}\n\n# combine plots using patchwork::wrap_plots\nall_plots <- patchwork::wrap_plots(plots, \n                                ncol = 1, \n                                guides=\"auto\") +\n                plot_annotation(title = \"Data from Subject 0: Scary-2 video\")\n\n# save the plots\nggsave(\"subject0-scaryvid.png\", all_plots, width=5, height=10)\n\n\n\nHm, just by looking, it doesn’t seem like there is a very clear relationship between the physiological measures and the emotion annotations. Near the end of the video, you see a bit of a spike in the EMG recordings (muscles) and GSR (skin conductance), which probably corresponded to a jumpy part in the scary clip. But let’s see what the linear regression model says."
  },
  {
    "objectID": "posts/001-RMSE/index.html#linear-regression-model",
    "href": "posts/001-RMSE/index.html#linear-regression-model",
    "title": "What does it mean to have an RMSE of 1.2? How wrong is it?",
    "section": "Linear regression model",
    "text": "Linear regression model\nWe will be fitting a linear regression model with the lm() function. (I also have, in the code chunk below, written a mixed effects model using lmer where I specified a varying intercept for each video, if you want to play around with that. I decided to keep it simple for this post.)\nMore importantly, we will train the model using the training dataset, and then evaluate the model using the test dataset. We will generate predictions for the test dataset and then calculate the RMSE for each subject for valence and arousal.\nThe formula for the linear regression model is: y ~ ecg + bvp + gsr + rsp + skt + emg_zygo + emg_coru + emg_trap; where y = valence and y = arousal separately.\n\n\nCode\n# train a model (mixed effects)\n# library(lme4)\n# m.arousal <- lmer(arousal ~ ecg + bvp + gsr + rsp + skt + emg_zygo + emg_coru + emg_trap + (1 | video), data=train_dataset)\n\n# m.valence <- lmer(valence ~ ecg + bvp + gsr + rsp + skt + emg_zygo + emg_coru + emg_trap + (1 | video), data=train_dataset)\n\n# train a model (linear regression)\nm.arousal <- lm(arousal ~ ecg + bvp + gsr + rsp + skt + emg_zygo + emg_coru + emg_trap, data=train_dataset)\n\nm.valence <- lm(valence ~ ecg + bvp + gsr + rsp + skt + emg_zygo + emg_coru + emg_trap, data=train_dataset)\n\n\nLet’s take a look at the model summary for arousal:\n\n\nCode\nlibrary(jtools)\n# summary of arousal model\nsumm(m.arousal, confint=TRUE, digits=3)\n\n\n\n\n\n  \n    Observations \n    158663 \n  \n  \n    Dependent variable \n    arousal \n  \n  \n    Type \n    OLS linear regression \n  \n\n \n\n  \n    F(8,158654) \n    1644.846 \n  \n  \n    R² \n    0.077 \n  \n  \n    Adj. R² \n    0.077 \n  \n\n \n \n  \n      \n    Est. \n    2.5% \n    97.5% \n    t val. \n    p \n  \n \n\n  \n    (Intercept) \n    3.054 \n    2.851 \n    3.256 \n    29.537 \n    0.000 \n  \n  \n    ecg \n    0.034 \n    0.014 \n    0.053 \n    3.428 \n    0.001 \n  \n  \n    bvp \n    0.025 \n    0.020 \n    0.030 \n    9.677 \n    0.000 \n  \n  \n    gsr \n    0.034 \n    0.034 \n    0.035 \n    95.250 \n    0.000 \n  \n  \n    rsp \n    -0.027 \n    -0.029 \n    -0.025 \n    -30.051 \n    0.000 \n  \n  \n    skt \n    0.042 \n    0.039 \n    0.044 \n    29.915 \n    0.000 \n  \n  \n    emg_zygo \n    0.033 \n    0.032 \n    0.035 \n    44.221 \n    0.000 \n  \n  \n    emg_coru \n    0.013 \n    0.012 \n    0.014 \n    30.656 \n    0.000 \n  \n  \n    emg_trap \n    0.004 \n    0.003 \n    0.004 \n    21.663 \n    0.000 \n  \n\n\n Standard errors: OLS\n\n\n\nAnd here’s the summary for valence:\n\n\nCode\n# summary of valence model\nsumm(m.valence, confint=TRUE, digits=3)\n\n\n\n\n\n  \n    Observations \n    158663 \n  \n  \n    Dependent variable \n    valence \n  \n  \n    Type \n    OLS linear regression \n  \n\n \n\n  \n    F(8,158654) \n    1881.978 \n  \n  \n    R² \n    0.087 \n  \n  \n    Adj. R² \n    0.087 \n  \n\n \n \n  \n      \n    Est. \n    2.5% \n    97.5% \n    t val. \n    p \n  \n \n\n  \n    (Intercept) \n    7.832 \n    7.614 \n    8.050 \n    70.367 \n    0.000 \n  \n  \n    ecg \n    -0.030 \n    -0.051 \n    -0.009 \n    -2.839 \n    0.005 \n  \n  \n    bvp \n    -0.025 \n    -0.030 \n    -0.020 \n    -9.092 \n    0.000 \n  \n  \n    gsr \n    -0.034 \n    -0.035 \n    -0.033 \n    -86.972 \n    0.000 \n  \n  \n    rsp \n    -0.015 \n    -0.017 \n    -0.013 \n    -15.267 \n    0.000 \n  \n  \n    skt \n    -0.022 \n    -0.025 \n    -0.019 \n    -14.640 \n    0.000 \n  \n  \n    emg_zygo \n    0.045 \n    0.043 \n    0.046 \n    55.045 \n    0.000 \n  \n  \n    emg_coru \n    -0.023 \n    -0.024 \n    -0.023 \n    -50.376 \n    0.000 \n  \n  \n    emg_trap \n    0.006 \n    0.006 \n    0.007 \n    35.300 \n    0.000 \n  \n\n\n Standard errors: OLS\n\n\n\nGreat! We have our models. The \\(R^2\\) suggests that <10% of the variance is explained by our models, even though all the variables are statistically significant. (When I played around with the mixed effects model with varying intercepts for each video, the variance explained went up to ~60%). Anyways, let’s generate predictions for the test dataset!"
  },
  {
    "objectID": "posts/001-RMSE/index.html#computing-the-rmse",
    "href": "posts/001-RMSE/index.html#computing-the-rmse",
    "title": "What does it mean to have an RMSE of 1.2? How wrong is it?",
    "section": "Computing the RMSE",
    "text": "Computing the RMSE\nWe will use the predict() function to generate predictions for the test dataset for each subject and each video. We will then calculate the mean RMSE for valence and arousal, averaged across all subjects.\n\n\nCode\n# subset test dataset into X_test and y_true\nX_test <- test_dataset %>%\n    select(-fold, -valence, -arousal)\n\ny_true <- test_dataset %>%\n    select(subject, video, time, valence, arousal)\n\n\n# use models to predict arousal and valence using test input\n\n# initialise counter\ncounter <- 0\nrmse_values_arousal <- list()\nrmse_values_valence <- list()\ntest_pred_arousal <- list()\ntest_pred_valence <- list()\n\n\nfor (sub in unique(X_test$subject)) {\n\n    for (vid in unique(X_test$video)) {\n\n        # add to counter\n        counter <- counter + 1\n\n        # subset the data\n        df <- subset(X_test, subject == sub & video == vid)\n\n        # subset the true y data\n        df_true_y <- subset(y_true, subject == sub & video == vid)\n\n        # predict arousal\n        pred_arousal <- predict(m.arousal, newdata = df)\n        # predict valence\n        pred_valence <- predict(m.valence, newdata = df)\n\n        # calculate RMSE\n        rmse_arousal <- sqrt(mean((y_true$arousal - pred_arousal)^2))\n        rmse_valence <- sqrt(mean((y_true$valence - pred_valence)^2))\n\n        # store RMSE values\n        rmse_values_arousal[[counter]] <- rmse_arousal\n        rmse_values_valence[[counter]] <- rmse_valence\n\n        # store predictions \n        test_pred_arousal[[counter]] <- pred_arousal\n        test_pred_valence[[counter]] <- pred_valence\n        \n    }\n}\n\naverage_rmse_arousal <- mean(unlist(rmse_values_arousal))\naverage_rmse_valence <- mean(unlist(rmse_values_valence))\n\n# print average RMSE values \npaste(\"Average RMSE Arousal:\", round(average_rmse_arousal, 3),\n      \"Average RMSE Valence:\", round(average_rmse_valence, 3), \n      collapse = \"\\n\")\n\n\n[1] \"Average RMSE Arousal: 1.881 Average RMSE Valence: 2.001\"\n\n\nWe got our RMSE values using this equation: sqrt(mean((y_true - y_pred)^2)). That is, we take the difference between the true value and the predicted value, square it, take the mean, and then take the square root. RMSE is the standard deviation of the residuals, and it tells us how far the true values are from our fitted regression line.\nSo what exactly does the RMSE score tell us? How “wrong” is our model? Is it wrong in many small ways or a few large ways? By squaring errors and calculating a mean, the RMSE penalises large errors.\nGoing back to my intro – where I said there were studies quoting an RMSE of 1.2 as “the best fitting model” – let’s see what our model of RMSE 1.8 actually looks like in terms of how well it really fits.\nLet’s plot the predicted valence and arousal ratings against the true ratings for each of our 8 participants per video. First, we’ll use scatter plots to see how well our predicted correlates with true ratings.\n\n\nCode\n# convert lists to data frames\ndf_pred <- data.frame(\n    valence_pred = unlist(test_pred_valence), \n    arousal_pred = unlist(test_pred_arousal)\n    )\n\ny_hat_y_true <- cbind(df_pred, y_true)\n\n\n\n\nCode\n# initialise empty list for storing plots\nplots <- list()\n# initialise counter\nsub_i <- 0\n\n# loop through subjects\nfor (sub in unique(y_hat_y_true$subject)) {\n\n  # add to counter\n  sub_i <- sub_i + 1\n  # initialise empty list for storing subject plots\n  subplots <- list()\n\n  # loop through videos (for each subject)\n    for(vid in unique(y_hat_y_true$video)){\n      \n      # subset data\n      subset <- y_hat_y_true %>% filter(subject == sub & video == vid)\n\n      # create scatter plot + add to list\n      subplots[[vid]] <- subset %>%\n        ggplot(aes(x=valence, y=valence_pred)) +\n        geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=\"lightgrey\") +\n        geom_jitter(width=0.05, height=0.05, color=\"#00aedb\", alpha=0.2) +\n        geom_jitter(aes(x=arousal, y=arousal_pred), width=0.05, height=0.05, color=\"#d11141\", alpha=0.2) +\n        ggtitle(paste0(\"Sub \", sub, \" : \", vid)) +\n        labs(x=\"True\", y=\"Predicted\") +\n        xlim(0,10) + ylim(0,10)\n    }\n\n    # arrange subject's plots in a column\n    plots[[sub_i]] <- patchwork::wrap_plots(subplots, ncol=1)\n\n}\n\n# arrange all subjects' plots in a grid\np <- patchwork::wrap_plots(plots, ncol=4) + \n  # add title\n  plot_annotation(title = \"Scatter Plot of Predicted and True Ratings\")\n\n# save plot\nggsave(\"pred_cor.png\", p, width=12, height=15)\n\n\n\nNext, we’ll look at how well our predicted ratings look against the true ratings over time.\n\n\nCode\n# initialise empty list for storing plots\nplots <- list()\n# initialise counter\nsub_i <- 0\n\n# loop through subjects\nfor (sub in unique(y_hat_y_true$subject)) {\n\n  # add to counter\n  sub_i <- sub_i + 1\n  # initialise empty list for storing subject plots\n  subplots <- list()\n\n    # loop through videos (for each subject)\n    for(vid in unique(y_hat_y_true$video)){\n\n      # subset data\n      subset <- y_hat_y_true %>%\n        filter(subject == sub & video == vid) %>%\n        pivot_longer(cols=c(valence, valence_pred, arousal, arousal_pred), names_to=\"annotations\", values_to=\"value\") %>%\n        mutate(annotations = factor(annotations))\n\n      # create line plot + add to list\n      subplots[[vid]] <- subset %>%\n        ggplot(aes(x=time, y=value, color=annotations)) +\n        geom_line(aes(linetype=annotations, color=annotations)) +\n        scale_linetype_manual(values=c(\"solid\", \"dotted\", \"solid\", \"dotted\"))+\n        scale_color_manual(values=c(\"#d11141\", \"#d11141\", \"#00aedb\", \"#00aedb\")) +\n        ggtitle(paste0(\"Sub \", sub, \" : \", vid)) +\n        theme(legend.position=\"right\")\n    }\n\n    # arrange subject's plots in a column\n    plots[[sub_i]] <- patchwork::wrap_plots(subplots, ncol=1)\n\n}\n\n# arrange all subjects' plots in a grid\np <- patchwork::wrap_plots(plots, ncol=4, guides=\"collect\") + \n  # add title\n  plot_annotation(title = \"Predicted vs True Ratings per Subject per Video\")\n\n# save plot\nggsave(\"pred_v_true.png\", p, width=15, height=15)\n\n\n\nSo, what do you think? Our RMSE for arousal was 1.8, which means that on average, our model was off by 1.8 points. But once we plotted it against the real values (especially over time), we see that our model is actually pretty awful. However, when the dataset gets large and the model gets more complex, it can be difficult to plot and visualise the model. In many ML applications, we often only ever look at one accuracy metric to determine whether a model is good at the task."
  },
  {
    "objectID": "posts/001-RMSE/index.html#conclusion",
    "href": "posts/001-RMSE/index.html#conclusion",
    "title": "What does it mean to have an RMSE of 1.2? How wrong is it?",
    "section": "Conclusion",
    "text": "Conclusion\nWhile it is important to quantitatively have metrics to measure how well our models are doing, it is still extremely important to look at the data and predictions qualitatively. Even though, for every observation, our model might only be wrong an average of 1.2 units (in the case of the RMSE = 1.2 model), if we string all these together to make hundreds of consecutive predictions, our model can be very wrong while still having a relatively “good” fit on average. So don’t be fooled by statistics and averages! And just because a model is termed “winning model” doesn’t mean it’s actually a good model, it just means the other models were worse.\n\n\nAcknowledgements\nSpecial thanks to the EPiC organisers for putting together such a fun competition, and to the CASE researchers who collected and made this dataset available. And thanks to Jason Geller for introducing me to Quarto blogs."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A blog about data, statistics, and machine learning.",
    "section": "",
    "text": "What does it mean to have an RMSE of 1.2? How wrong is it?\n\n\n\n\n\n\n\nR\n\n\nlinear regression\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2023\n\n\nJamie Chiu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Dear (Data) Diary is a blog about data science, machine learning, and statistics. Here, we try to answer questions like “Wait, how?” and “Why?” and “How did they come to that conclusion?”. Part musings part tutorials, we hope to provide a resource for those who are curious about the world around them and want to learn more about how numbers and data can help us understand it."
  }
]